{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioLM\n",
    "\n",
    "Implementation of <a href=\"https://google-research.github.io/seanet/audiolm/examples/\">AudioLM</a> in Pytorch Lightning.\n",
    "\n",
    "This implementation is based on [audiolm-pytorch](https://github.com/lucidrains/audiolm-pytorch). However, here we wrapped their model into a `LightningModule` in order to have ready-to-use object that sets up everything you need: the model but also optimizers, the training loop, etc.\n",
    "\n",
    "Hopefully, this repo is also easier to read and understand, both for users and developers that wish to contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | nb_init | Set current dir to synthetic-data\n",
      "INFO | nb_init | You are using Python 3.10.10 (main, Sep 14 2023, 16:59:47) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "from sai.utils import nb_init\n",
    "\n",
    "nb_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "\n",
    "from sai.datasets import MusicCaps\n",
    "from sai.models import AudioLMLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "AudioLM can be trained on the MusicCaps dataset. This is a dataset of YouTube audioclips with annotations.\n",
    "\n",
    "We will use a subset (`samples_to_load`) of total audio files, or the download will take time and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \".data/music_data\"\n",
    "\n",
    "# Load dataset\n",
    "dm = MusicCaps(\n",
    "    root=ROOT,\n",
    "    samples_to_load=32,\n",
    "    batch_size=1,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of this dataset comes in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ytid': ['-BHPu-dPmWQ'],\n",
       " 'start_s': tensor([30]),\n",
       " 'end_s': tensor([40]),\n",
       " 'audioset_positive_labels': ['/m/04rlf,/t/dd00032'],\n",
       " 'aspect_list': [\"['intimate wide mixed vocals', 'synth lead melody', 'punchy kick', 'noisy snare', 'claps', 'groovy bass guitar', 'tinny wide hi hats', 'short snare roll', 'alternative/indie', 'electric guitar melody', 'easygoing', 'melancholic']\"],\n",
       " 'caption': ['The Alternative/Indie song features an intimate, widely spread, mixed vocals singing over noisy snare, punchy kick, wide tinny hi hats, electric guitar melody, synth lead melody and groovy bass guitar. At the end of the loop there is a short snare roll and some claps. It sounds easygoing and melancholic thanks to those vocals.'],\n",
       " 'author_id': tensor([4]),\n",
       " 'is_balanced_subset': tensor([False]),\n",
       " 'is_audioset_eval': tensor([True]),\n",
       " 'audio': {'path': ['.data/music_data/-BHPu-dPmWQ.wav'],\n",
       "  'array': tensor([[ 0.0237,  0.0276,  0.0278,  ..., -0.3301, -0.4594,  0.0000]]),\n",
       "  'sampling_rate': tensor([44100])},\n",
       " 'download_status': tensor([True])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "`AudioLMLightning` initialization and training. `AudioLMLightning` will look in `data_folder` for audio files. This folder has been populated by the `MusicCaps` datamodule above.\n",
    "\n",
    "We may also have not initialized `MusicCaps` and just provided a `data_folder` to `AudioLMLightning`, `AudioLMLightning` would have downloaded the dataset for us.\n",
    "\n",
    "In the original [audiolm repo](https://github.com/lucidrains/audiolm-pytorch), one would need to download checkpoits, then initialize all models and transformers (`SoundStream`, etc.), train them one by one, then combining them together into a `AudioLM` object. You do not need to do this here. As you can see, by default everything is set up automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioLMLightning(data_folder=ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be training for a few steps only, for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    max_steps=4,\n",
    "    accelerator=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                         | Type                       | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | wave2vec                     | HubertWithKmeans           | 94.7 M\n",
      "1 | soundstream                  | SoundStream                | 48.8 M\n",
      "2 | semantic_transformer         | SemanticTransformer        | 59.2 M\n",
      "3 | semantic_transformer_wrapper | SemanticTransformerWrapper | 153 M \n",
      "4 | coarse_transformer           | CoarseTransformer          | 18.6 M\n",
      "5 | coarse_transformer_wrapper   | CoarseTransformerWrapper   | 162 M \n",
      "6 | fine_transformer             | FineTransformer            | 22.7 M\n",
      "7 | fine_transformer_wrapper     | FineTransformerWrapper     | 71.5 M\n",
      "8 | model                        | AudioLM                    | 243 M \n",
      "----------------------------------------------------------------------------\n",
      "243 M     Trainable params\n",
      "0         Non-trainable params\n",
      "243 M     Total params\n",
      "975.968   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d152dd5912149e09fc4550f0aa73ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d3c5e5a3e8477491b7f153d818eebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=4` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generat audio files. We may input some text or not at all.\n",
    "\n",
    "(This will take time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | forward | Generating semantic token...\n",
      "generating semantic:  61%|██████▏   | 628/1024 [00:07<00:04, 82.05it/s]\n",
      "INFO | forward | Generating coarse token...\n",
      "generating coarse: 100%|██████████| 512/512 [00:40<00:00, 12.62it/s]\n",
      "INFO | forward | Generating wave...\n",
      "generating fine: 100%|██████████| 512/512 [07:43<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_wav: torch.Tensor = model(\n",
    "    text='chirping of birds and the distant echos of bells',\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "PortAudioError",
     "evalue": "Error opening OutputStream: Invalid number of channels [PaErrorCode -9998]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/gianmarcoaversano/Documents/develop/synthetic-data/notebooks/AudioLM.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmarcoaversano/Documents/develop/synthetic-data/notebooks/AudioLM.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msounddevice\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gianmarcoaversano/Documents/develop/synthetic-data/notebooks/AudioLM.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sd\u001b[39m.\u001b[39;49mplay(generated_wav\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m44100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmarcoaversano/Documents/develop/synthetic-data/notebooks/AudioLM.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sd\u001b[39m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:175\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(data, samplerate, mapping, blocking, loop, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     ctx\u001b[39m.\u001b[39mwrite_outdata(outdata)\n\u001b[1;32m    173\u001b[0m     ctx\u001b[39m.\u001b[39mcallback_exit()\n\u001b[0;32m--> 175\u001b[0m ctx\u001b[39m.\u001b[39;49mstart_stream(OutputStream, samplerate, ctx\u001b[39m.\u001b[39;49moutput_channels,\n\u001b[1;32m    176\u001b[0m                  ctx\u001b[39m.\u001b[39;49moutput_dtype, callback, blocking,\n\u001b[1;32m    177\u001b[0m                  prime_output_buffers_using_stream_callback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m                  \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:2582\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_stream\u001b[39m(\u001b[39mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[1;32m   2580\u001b[0m                  blocking, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2581\u001b[0m     stop()  \u001b[39m# Stop previous playback/recording\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m StreamClass(samplerate\u001b[39m=\u001b[39;49msamplerate,\n\u001b[1;32m   2583\u001b[0m                               channels\u001b[39m=\u001b[39;49mchannels,\n\u001b[1;32m   2584\u001b[0m                               dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2585\u001b[0m                               callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m   2586\u001b[0m                               finished_callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfinished_callback,\n\u001b[1;32m   2587\u001b[0m                               \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   2589\u001b[0m     \u001b[39mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:1494\u001b[0m, in \u001b[0;36mOutputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, samplerate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, blocksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1465\u001b[0m              device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, channels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, latency\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m              extra_settings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, finished_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1467\u001b[0m              clip_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, never_drop_input\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1468\u001b[0m              prime_output_buffers_using_stream_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1469\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"PortAudio output stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \n\u001b[1;32m   1471\u001b[0m \u001b[39m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1494\u001b[0m     _StreamBase\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, kind\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m, wrap_callback\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39marray\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m   1495\u001b[0m                          \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_remove_self(\u001b[39mlocals\u001b[39;49m()))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:898\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    896\u001b[0m     userdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mNULL\n\u001b[1;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mnew(\u001b[39m'\u001b[39m\u001b[39mPaStream**\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 898\u001b[0m _check(_lib\u001b[39m.\u001b[39;49mPa_OpenStream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ptr, iparameters, oparameters,\n\u001b[1;32m    899\u001b[0m                           samplerate, blocksize, stream_flags,\n\u001b[1;32m    900\u001b[0m                           callback_ptr, userdata),\n\u001b[1;32m    901\u001b[0m        \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mError opening \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    903\u001b[0m \u001b[39m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:2747\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(err, msg)\u001b[0m\n\u001b[1;32m   2744\u001b[0m     hosterror_info \u001b[39m=\u001b[39m host_api, info\u001b[39m.\u001b[39merrorCode, hosterror_text\n\u001b[1;32m   2745\u001b[0m     \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err, hosterror_info)\n\u001b[0;32m-> 2747\u001b[0m \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error opening OutputStream: Invalid number of channels [PaErrorCode -9998]"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "sd.play(generated_wav.detach().cpu().numpy(), 44100)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
