{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioLM\n",
    "\n",
    "Implementation of <a href=\"https://google-research.github.io/seanet/audiolm/examples/\">AudioLM</a> in Pytorch Lightning.\n",
    "\n",
    "This implementation is based on [audiolm-pytorch](https://github.com/lucidrains/audiolm-pytorch). However, here we wrapped their model into a `LightningModule` in order to have ready-to-use object that sets up everything you need: the model but also optimizers, the training loop, etc.\n",
    "\n",
    "Hopefully, this repo is also easier to read and understand, both for users and developers that wish to contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | nb_init | Set current dir to synthetic-data\n",
      "INFO | nb_init | You are using Python 3.10.10 (main, Sep 14 2023, 16:59:47) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "from sai.utils import nb_init\n",
    "\n",
    "nb_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "from sai.datasets import MusicCaps\n",
    "from sai.models import AudioLMLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "AudioLM can be trained on the MusicCaps dataset. This is a dataset of YouTube audioclips with annotations.\n",
    "\n",
    "We will use a subset (`samples_to_load`) of total audio files, or the download will take time and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \".data/music_data\"\n",
    "\n",
    "# Load dataset\n",
    "dm = MusicCaps(\n",
    "    root=\".data/music_data\",\n",
    "    samples_to_load=32,\n",
    "    batch_size=1,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of this dataset comes in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ytid': ['-0vPFx-wRRI'],\n",
       " 'start_s': tensor([30]),\n",
       " 'end_s': tensor([40]),\n",
       " 'audioset_positive_labels': ['/m/025_jnm,/m/04rlf'],\n",
       " 'aspect_list': [\"['amateur recording', 'finger snipping', 'male mid range voice singing', 'reverb']\"],\n",
       " 'caption': ['a male voice is singing a melody with changing tempos while snipping his fingers rhythmically. The recording sounds like it has been recorded in an empty room. This song may be playing, practicing snipping and singing along.'],\n",
       " 'author_id': tensor([6]),\n",
       " 'is_balanced_subset': tensor([False]),\n",
       " 'is_audioset_eval': tensor([True]),\n",
       " 'audio': {'path': ['.data/music_data/-0vPFx-wRRI.wav'],\n",
       "  'array': tensor([[-0.0055, -0.0234, -0.0383,  ...,  0.0180, -0.0113,  0.0000]]),\n",
       "  'sampling_rate': tensor([44100])},\n",
       " 'download_status': tensor([True])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "`AudioLMLightning` initialization and training. `AudioLMLightning` will look in `data_folder` for audio files. This folder has been populated by the `MusicCaps` datamodule above.\n",
    "\n",
    "We may also have not initialized `MusicCaps` and just provided a `data_folder` to `AudioLMLightning`, `AudioLMLightning` would have downloaded the dataset for us.\n",
    "\n",
    "In the original [audiolm repo](https://github.com/lucidrains/audiolm-pytorch), one would need to download checkpoits, then initialize all models and transformers (`SoundStream`, etc.), train them one by one, then combining them together into a `AudioLM` object. You do not need to do this here. As you can see, by default everything is set up automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioLMLightning(data_folder=ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be training for a few steps only, for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    max_steps=4,\n",
    "    accelerator=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                         | Type                       | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | wave2vec                     | HubertWithKmeans           | 94.7 M\n",
      "1 | soundstream                  | SoundStream                | 48.8 M\n",
      "2 | semantic_transformer         | SemanticTransformer        | 59.2 M\n",
      "3 | semantic_transformer_wrapper | SemanticTransformerWrapper | 153 M \n",
      "4 | coarse_transformer           | CoarseTransformer          | 18.6 M\n",
      "5 | coarse_transformer_wrapper   | CoarseTransformerWrapper   | 162 M \n",
      "6 | fine_transformer             | FineTransformer            | 22.7 M\n",
      "7 | fine_transformer_wrapper     | FineTransformerWrapper     | 71.5 M\n",
      "8 | model                        | AudioLM                    | 243 M \n",
      "----------------------------------------------------------------------------\n",
      "243 M     Trainable params\n",
      "0         Non-trainable params\n",
      "243 M     Total params\n",
      "975.968   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e0b1e851364f20bbcf95caa9c0026a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9f15fbd32b470fab78ff8196f088f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=4` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generat audio files. We may input some text or not at all.\n",
    "\n",
    "(This will take time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | forward | Generating semantic token...\n",
      "generating semantic:  38%|███▊      | 776/2048 [00:10<00:17, 73.85it/s]\n",
      "INFO | forward | Generating coarse token...\n",
      "generating coarse: 100%|██████████| 512/512 [00:53<00:00,  9.52it/s]\n",
      "INFO | forward | Generating wave...\n"
     ]
    }
   ],
   "source": [
    "generated_wav = model(text='chirping of birds and the distant echos of bells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
