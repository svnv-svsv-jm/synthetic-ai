{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioLM\n",
    "\n",
    "Implementation of <a href=\"https://google-research.github.io/seanet/audiolm/examples/\">AudioLM</a> in Pytorch Lightning.\n",
    "\n",
    "This implementation is based on [audiolm-pytorch](https://github.com/lucidrains/audiolm-pytorch). However, here we wrapped their model into a `LightningModule` in order to have ready-to-use object that sets up everything you need: the model but also optimizers, the training loop, etc.\n",
    "\n",
    "Hopefully, this repo is also easier to read and understand, both for users and developers that wish to contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:28.080493Z",
     "iopub.status.busy": "2024-01-02T16:39:28.080337Z",
     "iopub.status.idle": "2024-01-02T16:39:28.097262Z",
     "shell.execute_reply": "2024-01-02T16:39:28.096856Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:28.099847Z",
     "iopub.status.busy": "2024-01-02T16:39:28.099486Z",
     "iopub.status.idle": "2024-01-02T16:39:32.794984Z",
     "shell.execute_reply": "2024-01-02T16:39:32.794620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | nb_init | Set current dir to synthetic-data\n",
      "INFO | nb_init | You are using Python 3.10.10 (main, Sep 14 2023, 16:59:47) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "from sai.utils import nb_init\n",
    "\n",
    "nb_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:32.797201Z",
     "iopub.status.busy": "2024-01-02T16:39:32.796955Z",
     "iopub.status.idle": "2024-01-02T16:39:35.693720Z",
     "shell.execute_reply": "2024-01-02T16:39:35.693284Z"
    }
   },
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "\n",
    "from sai.datasets import MusicCaps\n",
    "from sai.models import AudioLMLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "AudioLM can be trained on the MusicCaps dataset. This is a dataset of YouTube audioclips with annotations.\n",
    "\n",
    "We will use a subset (`samples_to_load`) of total audio files, or the download will take time and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:35.696433Z",
     "iopub.status.busy": "2024-01-02T16:39:35.695849Z",
     "iopub.status.idle": "2024-01-02T16:39:38.093204Z",
     "shell.execute_reply": "2024-01-02T16:39:38.092849Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = \".data/music_data\"\n",
    "\n",
    "# Load dataset\n",
    "dm = MusicCaps(\n",
    "    root=ROOT,\n",
    "    samples_to_load=32,\n",
    "    batch_size=1,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of this dataset comes in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:38.095288Z",
     "iopub.status.busy": "2024-01-02T16:39:38.095119Z",
     "iopub.status.idle": "2024-01-02T16:39:43.674614Z",
     "shell.execute_reply": "2024-01-02T16:39:43.673512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ytid': ['-CUp_Tmg2Y0'],\n",
       " 'start_s': tensor([30]),\n",
       " 'end_s': tensor([40]),\n",
       " 'audioset_positive_labels': ['/m/01qbl,/m/026t6,/m/02hnl,/m/03qtq,/m/03t3fj,/m/0bm02,/m/0l14md'],\n",
       " 'aspect_list': [\"['low quality', 'mono', 'noisy', 'drums solo', 'double pedal kick', 'punchy snare', 'shimmering cymbals', 'boomy toms', 'energetic', 'manic']\"],\n",
       " 'caption': ['The low quality recording features a drum solo that consists of a double pedal kick, punchy snare, shimmering cymbals and boomy toms. It sounds energetic and manic, thanks to that kick pattern. The recording is mono and noisy.'],\n",
       " 'author_id': tensor([4]),\n",
       " 'is_balanced_subset': tensor([False]),\n",
       " 'is_audioset_eval': tensor([False]),\n",
       " 'audio': {'path': ['.data/music_data/-CUp_Tmg2Y0.wav'],\n",
       "  'array': tensor([[0.0067, 0.0095, 0.0081,  ..., 0.0173, 0.0243, 0.0000]]),\n",
       "  'sampling_rate': tensor([44100])},\n",
       " 'download_status': tensor([True])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "`AudioLMLightning` initialization and training. `AudioLMLightning` will look in `data_folder` for audio files. This folder has been populated by the `MusicCaps` datamodule above.\n",
    "\n",
    "We may also have not initialized `MusicCaps` and just provided a `data_folder` to `AudioLMLightning`, `AudioLMLightning` would have downloaded the dataset for us.\n",
    "\n",
    "In the original [audiolm repo](https://github.com/lucidrains/audiolm-pytorch), one would need to download checkpoits, then initialize all models and transformers (`SoundStream`, etc.), train them one by one, then combining them together into a `AudioLM` object. You do not need to do this here. As you can see, by default everything is set up automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:43.678071Z",
     "iopub.status.busy": "2024-01-02T16:39:43.677571Z",
     "iopub.status.idle": "2024-01-02T16:39:49.093245Z",
     "shell.execute_reply": "2024-01-02T16:39:49.092868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 17:46:24 | INFO | fairseq.tasks.hubert_pretraining | current directory is /Users/gianmarcoaversano/Documents/develop/synthetic-data\n",
      "2024-01-02 17:46:24 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-01-02 17:46:24 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n"
     ]
    }
   ],
   "source": [
    "model = AudioLMLightning(data_folder=ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be training for a few steps only, for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:49.095830Z",
     "iopub.status.busy": "2024-01-02T16:39:49.095657Z",
     "iopub.status.idle": "2024-01-02T16:39:49.316943Z",
     "shell.execute_reply": "2024-01-02T16:39:49.316568Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    max_steps=4,\n",
    "    accelerator=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:39:49.319240Z",
     "iopub.status.busy": "2024-01-02T16:39:49.319064Z",
     "iopub.status.idle": "2024-01-02T16:40:33.102617Z",
     "shell.execute_reply": "2024-01-02T16:40:33.102206Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                         | Type                       | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | wave2vec                     | HubertWithKmeans           | 94.7 M\n",
      "1 | soundstream                  | SoundStream                | 48.8 M\n",
      "2 | semantic_transformer         | SemanticTransformer        | 59.2 M\n",
      "3 | semantic_transformer_wrapper | SemanticTransformerWrapper | 153 M \n",
      "4 | coarse_transformer           | CoarseTransformer          | 18.6 M\n",
      "5 | coarse_transformer_wrapper   | CoarseTransformerWrapper   | 162 M \n",
      "6 | fine_transformer             | FineTransformer            | 22.7 M\n",
      "7 | fine_transformer_wrapper     | FineTransformerWrapper     | 71.5 M\n",
      "8 | model                        | AudioLM                    | 243 M \n",
      "----------------------------------------------------------------------------\n",
      "243 M     Trainable params\n",
      "0         Non-trainable params\n",
      "243 M     Total params\n",
      "975.968   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd26cac47dbf40759b34cf7c2ed36e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882665fca2804f07aa182b8d0ec3d727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=4` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generat audio files. We may input some text or not at all.\n",
    "\n",
    "(This will take time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T16:40:33.105451Z",
     "iopub.status.busy": "2024-01-02T16:40:33.105257Z",
     "iopub.status.idle": "2024-01-02T16:45:33.993884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | forward | Generating semantic token...\n",
      "generating semantic:  23%|██▎       | 234/1024 [00:02<00:09, 83.54it/s]\n",
      "INFO | forward | Generating coarse token...\n",
      "generating coarse: 100%|██████████| 512/512 [00:38<00:00, 13.37it/s]\n",
      "INFO | forward | Generating wave...\n",
      "generating fine: 100%|██████████| 512/512 [06:24<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_wav: torch.Tensor = model(\n",
    "    text='chirping of birds and the distant echos of bells',\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PortAudioError",
     "evalue": "Error opening OutputStream: Invalid number of channels [PaErrorCode -9998]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msounddevice\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_wav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m44100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sd\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:175\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(data, samplerate, mapping, blocking, loop, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_outdata(outdata)\n\u001b[1;32m    173\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mcallback_exit()\n\u001b[0;32m--> 175\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOutputStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprime_output_buffers_using_stream_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:2582\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[1;32m   2580\u001b[0m                  blocking, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2581\u001b[0m     stop()  \u001b[38;5;66;03m# Stop previous playback/recording\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[43mStreamClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinished_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinished_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:1494\u001b[0m, in \u001b[0;36mOutputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1465\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1467\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1468\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio output stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \n\u001b[1;32m   1471\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1494\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:898\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    896\u001b[0m     userdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaStream**\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 898\u001b[0m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_OpenStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallback_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/sai/lib/python3.10/site-packages/sounddevice.py:2747\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(err, msg)\u001b[0m\n\u001b[1;32m   2744\u001b[0m     hosterror_info \u001b[38;5;241m=\u001b[39m host_api, info\u001b[38;5;241m.\u001b[39merrorCode, hosterror_text\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(errormsg, err, hosterror_info)\n\u001b[0;32m-> 2747\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(errormsg, err)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error opening OutputStream: Invalid number of channels [PaErrorCode -9998]"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "sd.play(generated_wav.detach().cpu().numpy(), 44100)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
